{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "source": "from io import BytesIO\nfrom azureml.serialization import (\n    DataTypeIds,\n    deserialize_dataframe,\n    serialize_dataframe,\n    is_supported,\n)\n\nimport json\nimport requests\nfrom azureml.errors import AzureMLConflictHttpError\n\ntry:\n    from urlparse import urljoin\nexcept ImportError:\n    from urllib.parse import urljoin\n\nfrom azureml.errors import (\n    AzureMLHttpError,\n)\n\n__author__ = 'Microsoft Corp. <ptvshelp@microsoft.com>'\n__version__ = '0.2.6'\n\n\nclass _RestClient(object):\n    SERVICE_ROOT = 'api/'\n    INTERMEDIATE_DATASET_URI_FMT = SERVICE_ROOT + 'workspaces/{0}/experiments/{1}/outputdata/{2}/{3}'\n    EXPERIMENTS_URI_FMT = SERVICE_ROOT + 'workspaces/{0}/experiments'\n    DATASOURCES_URI_FMT = SERVICE_ROOT + 'workspaces/{0}/datasources'\n    DATASOURCE_URI_FMT = SERVICE_ROOT + 'workspaces/{0}/datasources/{1}'\n    UPLOAD_URI_FMI = SERVICE_ROOT + 'resourceuploads/workspaces/{0}/?userStorage=true&dataTypeId={1}'\n    UPLOAD_CHUNK_URI_FMT = SERVICE_ROOT + 'blobuploads/workspaces/{0}/?numberOfBlocks={1}&blockId={2}&uploadId={3}&dataTypeId={4}'\n    SESSION_ID_HEADER_NAME = 'x-ms-client-session-id'\n    SESSION_ID_HEADER_VALUE = 'DefaultSession'\n    ACCESS_TOKEN_HEADER_NAME = 'x-ms-metaanalytics-authorizationtoken'\n    CONTENT_TYPE_HEADER_NAME = 'Content-Type'\n    CONTENT_TYPE_HEADER_VALUE_JSON = 'application/json;charset=UTF8'\n    CHUNK_SIZE = 0x200000 \n    DEFAULT_OWNER = 'Python SDK'\n    USER_AGENT_HEADER_NAME = 'User-Agent'\n    USER_AGENT_HEADER_VALUE = 'pyazureml/' + __version__\n\n    def __init__(self, service_endpoint, access_token):\n        self._service_endpoint = service_endpoint\n        self._access_token = access_token\n\n    def get_experiments(self, workspace_id):\n        \"\"\"Runs HTTP GET request to retrieve the list of experiments.\"\"\"\n        api_path = self.EXPERIMENTS_URI_FMT.format(workspace_id)\n        return self._send_get_req(api_path)\n\n    def get_datasets(self, workspace_id):\n        \"\"\"Runs HTTP GET request to retrieve the list of datasets.\"\"\"\n        api_path = self.DATASOURCES_URI_FMT.format(workspace_id)\n        return self._send_get_req(api_path)\n\n    def get_dataset(self, workspace_id, dataset_id):\n        \"\"\"Runs HTTP GET request to retrieve a single dataset.\"\"\"\n        api_path = self.DATASOURCE_URI_FMT.format(workspace_id, dataset_id)\n        return self._send_get_req(api_path)\n\n    def open_intermediate_dataset_contents(self, workspace_id, experiment_id,\n                                           node_id, port_name):\n        return self._get_intermediate_dataset_contents(\n            workspace_id,\n            experiment_id,\n            node_id,\n            port_name,\n            stream=True).raw\n\n    def read_intermediate_dataset_contents_binary(self, workspace_id,\n                                                  experiment_id, node_id,\n                                                  port_name):\n        return self._get_intermediate_dataset_contents(\n            workspace_id,\n            experiment_id,\n            node_id,\n            port_name,\n            stream=False).content\n\n    def read_intermediate_dataset_contents_text(self, workspace_id,\n                                                experiment_id, node_id,\n                                                port_name):\n        return self._get_intermediate_dataset_contents(\n            workspace_id,\n            experiment_id,\n            node_id,\n            port_name,\n            stream=False).text\n\n    def _get_intermediate_dataset_contents(self, workspace_id, experiment_id,\n                                           node_id, port_name, stream):\n        api_path = self.INTERMEDIATE_DATASET_URI_FMT.format(\n            workspace_id, experiment_id, node_id, port_name)\n        response = requests.get(\n            url=urljoin(self._service_endpoint, api_path),\n            headers=self._get_headers(),\n            stream=stream,\n        )\n        return response\n\n    def open_dataset_contents(self, url):\n        response = requests.get(url, stream=True)\n        return response.raw\n\n    def read_dataset_contents_binary(self, url):\n        response = requests.get(url)\n        return response.content\n\n    def read_dataset_contents_text(self, url):\n        response = requests.get(url)\n        return response.text\n\n    def upload_dataset(self, workspace_id, name, description, data_type_id,\n                       raw_data, family_id):\n        # uploading data is a two step process. First we upload the raw data\n        api_path = self.UPLOAD_URI_FMI.format(workspace_id, data_type_id)\n        upload_result = self._send_post_req(api_path, data=b'')\n\n        # now get the id that was generated\n        upload_id = upload_result[\"Id\"]\n\n        # Upload the data in chunks...\n        total_chunks = int((len(raw_data) + (self.CHUNK_SIZE-1)) / self.CHUNK_SIZE)\n        for chunk in range(total_chunks):\n            chunk_url = self.UPLOAD_CHUNK_URI_FMT.format(\n                workspace_id,\n                total_chunks, # number of blocks\n                chunk, # block id\n                upload_id,\n                data_type_id,\n            )\n            chunk_data = raw_data[chunk*self.CHUNK_SIZE:(chunk + 1)*self.CHUNK_SIZE]\n            self._send_post_req(chunk_url, data=chunk_data)\n\n        # use that to construct the DataSource metadata\n        metadata = {\n            \"DataSource\": {\n                \"Name\": name,\n                \"DataTypeId\":data_type_id,\n                \"Description\":description,\n                \"FamilyId\":family_id,\n                \"Owner\": self.DEFAULT_OWNER,\n                \"SourceOrigin\":\"FromResourceUpload\"\n            },\n            \"UploadId\": upload_id,\n            \"UploadedFromFileName\":\"\",\n            \"ClientPoll\": True\n        }\n\n        try:\n            api_path = self.DATASOURCES_URI_FMT.format(workspace_id)\n        except AzureMLConflictHttpError as e:\n            raise AzureMLConflictHttpError(\n                'A data set named \"{}\" already exists'.format(name), \n                e.status_code\n            )\n\n        datasource_id = self._send_post_req(\n            api_path, json.dumps(metadata), self.CONTENT_TYPE_HEADER_VALUE_JSON)\n        return datasource_id\n\n    def _send_get_req(self, api_path):\n        response = requests.get(\n            url=urljoin(self._service_endpoint, api_path),\n            headers=self._get_headers()\n        )\n\n        if response.status_code >= 400:\n            raise AzureMLHttpError(response.text, response.status_code)\n\n        return response.json()\n\n    def _send_post_req(self, api_path, data, content_type=None):\n        response = requests.post(\n            url=urljoin(self._service_endpoint, api_path),\n            data=data,\n            headers=self._get_headers(content_type)\n        )\n\n        if response.status_code >= 400:\n            raise AzureMLHttpError(response.text, response.status_code)\n\n        return response.json()\n\n    def _get_headers(self, content_type=None):\n        headers = {\n            self.USER_AGENT_HEADER_NAME: self.USER_AGENT_HEADER_VALUE,\n            self.CONTENT_TYPE_HEADER_NAME: self.CONTENT_TYPE_HEADER_VALUE_JSON,\n            self.SESSION_ID_HEADER_NAME: self.SESSION_ID_HEADER_VALUE,\n            self.ACCESS_TOKEN_HEADER_NAME: self._access_token\n        }\n        if content_type:\n            headers[self.CONTENT_TYPE_HEADER_NAME] = content_type\n        return headers ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 2, "cell_type": "code", "source": "def save_csv_to_Azure(data, workspace_id, authorization_token, name, description):\n    rc = _RestClient('https://studioapi.azureml.net', authorization_token)\n    output = BytesIO()\n    serialize_dataframe(output, 'GenericCSV', data)\n    raw_data = output.getvalue()\n    output.close()\n    \n    rc.upload_dataset(workspace_id=workspace_id, \n                      name=name, \n                      description=description,\n                      data_type_id='GenericCSV',\n                      raw_data=raw_data,\n                      family_id=None)   ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# example\nsave_csv_to_Azure(data, my_workspace_id, my_token, name, description)", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.4.4", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}
